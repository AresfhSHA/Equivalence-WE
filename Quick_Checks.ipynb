{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from corpus_raws import corpus_processing\n",
    "\n",
    "corpus = corpus_processing('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(corpus)) \n",
    "corpus[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to creat an array or dictionary to compute the co-occurence matrix as in the GloVe Algorithm/paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "window_size = 2\n",
    "from collections import defaultdict\n",
    "\n",
    "# Create co-occurrence matrix\n",
    "co_occurrence_matrix = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "\n",
    "for i, target_word in enumerate(corpus):\n",
    "    start = max(0, i - window_size)\n",
    "    end = min(len(corpus), i + window_size + 1)\n",
    "    context_words = corpus[start:end]\n",
    "    context_words.remove(target_word)\n",
    "    for context_word in context_words:\n",
    "        #print(target_word)\n",
    "        #print(context_word)\n",
    "        co_occurrence_matrix[target_word][context_word] += 1\n",
    "\n",
    "print(type(co_occurrence_matrix))\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Optionally, convert the matrix to a numpy array for further analysis\n",
    "unique_words = list(set(corpus))\n",
    "matrix_size = len(unique_words)\n",
    "matrix = np.zeros((matrix_size, matrix_size))\n",
    "\n",
    "word_to_index = {word: i for i, word in enumerate(unique_words)}\n",
    "print(type(co_occurrence_matrix))\n",
    "\n",
    "for target_word, context_words in co_occurrence_matrix.items():\n",
    "    for context_word, count in context_words.items():\n",
    "        matrix[word_to_index[target_word]][word_to_index[context_word]] = count\n",
    "\n",
    "print(\"\\nCo-occurrence matrix as a numpy array:\")\n",
    "print(type(matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming you have a matrix named 'matrix'\n",
    "matrix = np.array([[1, 2, 3],\n",
    "                   [4, 5, 6],\n",
    "                   [7, 8, 9]])\n",
    "\n",
    "# Calculate the norm of each row\n",
    "row_norms = np.linalg.norm(matrix, ord=1, axis=1, keepdims=True)\n",
    "print(row_norms)\n",
    "\n",
    "# Divide each entry by its row's norm\n",
    "normalized_matrix = matrix / row_norms\n",
    "\n",
    "print(\"Original Matrix:\")\n",
    "print(type(matrix))\n",
    "\n",
    "print(\"\\nNormalized Matrix:\")\n",
    "print(normalized_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_zeros = not np.any(matrix)\n",
    "print(all_zeros)\n",
    "\n",
    "np.all(matrix==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "embedding_size = 50\n",
    "learning_rate = 0.1\n",
    "epochs = 100\n",
    "\n",
    "# Initialize word vectors\n",
    "word_vectors = np.eye(len(unique_words), embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "word_vectors = torch.tensor(word_vectors, requires_grad=True)\n",
    "learning_rate = 0.01\n",
    "epochs = 25\n",
    "batch_size = 64  # Adjust as needed\n",
    "\n",
    "optimizer = torch.optim.SGD([word_vectors], lr=learning_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    for target_word, context_words in co_occurrence_matrix.items():\n",
    "        for context_word, count in context_words.items():\n",
    "            i = word_to_index[target_word]\n",
    "            j = word_to_index[context_word]\n",
    "\n",
    "            dot_product = torch.dot(word_vectors[i], word_vectors[j])\n",
    "            log_co_occurrence = np.log(count)\n",
    "\n",
    "            loss = (dot_product - log_co_occurrence)**2\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if (i + 1) % batch_size == 0:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming you already have tensors word_vectors, co_occurrence_matrix, etc.\n",
    "\n",
    "word_vectors = torch.tensor(word_vectors, requires_grad=True)\n",
    "\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    for target_word, context_words in co_occurrence_matrix.items():\n",
    "        for context_word, count in context_words.items():\n",
    "            i = word_to_index[target_word]\n",
    "            j = word_to_index[context_word]\n",
    "\n",
    "            # Dot product and log co-occurrence count\n",
    "            dot_product = torch.dot(word_vectors[i], word_vectors[j])\n",
    "            log_co_occurrence = np.log(count)  \n",
    "\n",
    "            # Loss function\n",
    "            loss = (dot_product - log_co_occurrence)**2\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Update word vectors using gradient descent\n",
    "            loss.backward()  # Backpropagation\n",
    "            with torch.no_grad():\n",
    "                word_vectors -= learning_rate * word_vectors.grad\n",
    "                word_vectors.grad.zero_()  # Zero gradients for the next iteration\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trained word vectors\n",
    "print(\"Trained Word Vectors:\")\n",
    "print(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch\n",
    "\n",
    "#co_occurrence_tensor = torch.Tensor([[co_occurrence_matrix[target_word].get(context_word, 0) for context_word in unique_words] for target_word in unique_words])\n",
    "#print(co_occurrence_tensor)\n",
    "#print(unique_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Assuming co_occurrence_matrix is already defined\n",
    "\n",
    "# Set hyperparameters\n",
    "embedding_size = 50\n",
    "learning_rate = 0.05\n",
    "epochs = 100\n",
    "\n",
    "# Convert co-occurrence matrix to torch tensors\n",
    "unique_words = list(set(word for target_word, context_words in co_occurrence_matrix.items() for word in [target_word] + list(context_words.keys())))\n",
    "word_to_index = {word: idx for idx, word in enumerate(unique_words)}\n",
    "\n",
    "print(f\"Target words: {unique_words}\")\n",
    "\n",
    "print(f\"Context words: {unique_words}\")\n",
    "\n",
    "#co_occurrence_tensor = torch.Tensor([[co_occurrence_matrix[target_word].get(context_word, 0) for context_word in unique_words] for target_word in unique_words])\n",
    "co_occurrence_tensor = torch.Tensor([\n",
    "    [\n",
    "        co_occurrence_matrix[target_word].get(context_word, 0) \n",
    "        for context_word in unique_words[:-1]  # Exclude the last word\n",
    "    ] + [\n",
    "        co_occurrence_matrix[target_word].get(unique_words[-1], 0)  # Use the last word separately\n",
    "    ] \n",
    "    for target_word in unique_words\n",
    "])\n",
    "\n",
    "class GloVeModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(GloVeModel, self).__init__()\n",
    "        self.target_embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.context_embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.bias_target = nn.Embedding(vocab_size, 1)\n",
    "        self.bias_context = nn.Embedding(vocab_size, 1)\n",
    "\n",
    "    def forward(self, i, j):\n",
    "        target_embedding = self.target_embeddings(i)\n",
    "        context_embedding = self.context_embeddings(j)\n",
    "        bias_target = self.bias_target(i).squeeze()\n",
    "        bias_context = self.bias_context(j).squeeze()\n",
    "        dot_product = torch.sum(target_embedding * context_embedding, dim=1)\n",
    "        return dot_product + bias_target + bias_context\n",
    "\n",
    "# Initialize model and optimizer\n",
    "glove_model = GloVeModel(len(unique_words), embedding_size)\n",
    "optimizer = optim.Adam(glove_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    i, j = torch.nonzero(co_occurrence_tensor, as_tuple=True)\n",
    "    targets = torch.log(co_occurrence_tensor[i, j])\n",
    "    predictions = glove_model(i, j)\n",
    "    loss = nn.functional.mse_loss(predictions, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "# Trained word vectors\n",
    "trained_word_vectors = glove_model.target_embeddings.weight.detach().numpy()\n",
    "print(\"Trained Word Vectors:\")\n",
    "print(trained_word_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "co_occurrence_tensor = torch.Tensor([\n",
    "    [\n",
    "        co_occurrence_matrix[target_word].get(context_word, 0) \n",
    "        for context_word in unique_words[:-1]  # Exclude the last word\n",
    "    ] + [\n",
    "        co_occurrence_matrix[target_word].get(unique_words[-1], 0)  # Use the last word separately\n",
    "    ] \n",
    "    for target_word in unique_words\n",
    "])\n",
    "\n",
    "print(co_occurrence_tensor)\n",
    "print(unique_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occurrence_tensor = torch.Tensor([[co_occurrence_matrix[target_word].get(context_word, 0) for context_word in unique_words] for target_word in unique_words])\n",
    "print(co_occurrence_tensor)\n",
    "print(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'tree', 'the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'tree']\n"
     ]
    }
   ],
   "source": [
    "Text = \"The quick brown fox jumps over the tree. The quick brown fox jumps over the tree.\"\n",
    "\n",
    "Text = Text.lower()\n",
    "Text = Text.replace(\".\",\"\")\n",
    "tokens = Text.split()\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the 0-th position corresponding to the word: the\n",
      "This is the 1-th position corresponding to the word: quick\n",
      "This is the 2-th position corresponding to the word: brown\n",
      "This is the 3-th position corresponding to the word: fox\n",
      "This is the 4-th position corresponding to the word: jumps\n",
      "This is the 5-th position corresponding to the word: over\n",
      "This is the 6-th position corresponding to the word: the\n",
      "This is the 7-th position corresponding to the word: tree\n",
      "This is the 8-th position corresponding to the word: the\n",
      "This is the 9-th position corresponding to the word: quick\n",
      "This is the 10-th position corresponding to the word: brown\n",
      "This is the 11-th position corresponding to the word: fox\n",
      "This is the 12-th position corresponding to the word: jumps\n",
      "This is the 13-th position corresponding to the word: over\n",
      "This is the 14-th position corresponding to the word: the\n",
      "This is the 15-th position corresponding to the word: tree\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, target_word in enumerate(tokens):\n",
    "    print(f\"This is the {i}-th position corresponding to the word: {target_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'tree', 'the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'tree']\n",
      "['quick', 'brown']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing GloVe Co-occurrence Matrix: 16it [00:00, 98544.59it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(<function Cat_Dist_GloVe.compute_co_occurrence_matrix.<locals>.<lambda>()>,\n",
       "            {'the': defaultdict(int,\n",
       "                         {'quick': 2,\n",
       "                          'brown': 2,\n",
       "                          'jumps': 2,\n",
       "                          'over': 2,\n",
       "                          'tree': 3,\n",
       "                          'the': 2}),\n",
       "             'quick': defaultdict(int,\n",
       "                         {'the': 2, 'brown': 2, 'fox': 2, 'tree': 1}),\n",
       "             'brown': defaultdict(int,\n",
       "                         {'the': 2, 'quick': 2, 'fox': 2, 'jumps': 2}),\n",
       "             'fox': defaultdict(int,\n",
       "                         {'quick': 2, 'brown': 2, 'jumps': 2, 'over': 2}),\n",
       "             'jumps': defaultdict(int,\n",
       "                         {'brown': 2, 'fox': 2, 'over': 2, 'the': 2}),\n",
       "             'over': defaultdict(int,\n",
       "                         {'fox': 2, 'jumps': 2, 'the': 2, 'tree': 2}),\n",
       "             'tree': defaultdict(int, {'over': 2, 'the': 3, 'quick': 1})})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Cat_Dist_GloVe import (\n",
    "    compute_context_words,\n",
    "    compute_co_occurrence_matrix,\n",
    "    defaultdict_to_ndarray,\n",
    "    compute_GloVe_probability_matrix,\n",
    "    probability_to_distance,\n",
    ")\n",
    "print(tokens)\n",
    "print(compute_context_words(tokens,0,\"the\", 2))\n",
    "#print(compute_context_words(tokens,9,\"quick\", 2))\n",
    "compute_co_occurrence_matrix(tokens, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brown', 'fox', 'jumps', 'over', 'quick', 'the', 'tree']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing GloVe Co-occurrence Matrix: 16it [00:00, 75573.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0 2 2 2 2 3]\n",
      " [2 2 0 0 0 2 1]\n",
      " [0 2 2 0 2 2 0]\n",
      " [2 0 2 2 2 0 0]\n",
      " [2 2 0 2 0 2 0]\n",
      " [0 2 2 0 0 2 2]\n",
      " [0 0 0 2 1 3 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dict = compute_co_occurrence_matrix(tokens, 2)\n",
    "\n",
    "X = defaultdict_to_ndarray(my_dict)\n",
    "print(X)\n",
    "len(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.15384615 0.         0.15384615 0.15384615 0.15384615 0.15384615\n",
      "  0.23076923]\n",
      " [0.28571429 0.28571429 0.         0.         0.         0.28571429\n",
      "  0.14285714]\n",
      " [0.         0.25       0.25       0.         0.25       0.25\n",
      "  0.        ]\n",
      " [0.25       0.         0.25       0.25       0.25       0.\n",
      "  0.        ]\n",
      " [0.25       0.25       0.         0.25       0.         0.25\n",
      "  0.        ]\n",
      " [0.         0.25       0.25       0.         0.         0.25\n",
      "  0.25      ]\n",
      " [0.         0.         0.         0.33333333 0.16666667 0.5\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prob_array = compute_GloVe_probability_matrix(X)\n",
    "print(prob_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.87180218        inf 1.87180218 1.87180218 1.87180218 1.87180218\n",
      "  1.46633707]\n",
      " [1.25276297 1.25276297        inf        inf        inf 1.25276297\n",
      "  1.94591015]\n",
      " [       inf 1.38629436 1.38629436        inf 1.38629436 1.38629436\n",
      "         inf]\n",
      " [1.38629436        inf 1.38629436 1.38629436 1.38629436        inf\n",
      "         inf]\n",
      " [1.38629436 1.38629436        inf 1.38629436        inf 1.38629436\n",
      "         inf]\n",
      " [       inf 1.38629436 1.38629436        inf        inf 1.38629436\n",
      "  1.38629436]\n",
      " [       inf        inf        inf 1.09861229 1.79175947 0.69314718\n",
      "         inf]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/4070544070543F3A/Trabajo/Doctorado/Word Embeddings/Divergence Code/Cat_Dist_GloVe.py:70: RuntimeWarning: divide by zero encountered in log\n",
      "  #---Maybe reference the paper for inspiration---#\n"
     ]
    }
   ],
   "source": [
    "dist_array = probability_to_distance(prob_array)\n",
    "print(dist_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.]\n",
      " [ 0.]\n",
      " [ 2.]]\n",
      "[[-0.28446415]\n",
      " [ 0.57521624]\n",
      " [-0.29494405]]\n",
      "0.0\n",
      "0.005335514101335329\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "dimension = 1\n",
    "\n",
    "\n",
    "from sklearn.manifold import MDS\n",
    "mdsD = MDS(n_components=dimension, \n",
    "              metric=True, \n",
    "              normalized_stress=False,   \n",
    "              dissimilarity='precomputed'\n",
    "              )\n",
    "\n",
    "mdsP = MDS(n_components=dimension, \n",
    "              metric=False, \n",
    "              normalized_stress=True,   \n",
    "              dissimilarity='precomputed'\n",
    "              )\n",
    "\n",
    "D = np.array([[0, 2, 4],\n",
    "             [2, 0, 2],\n",
    "             [4, 2, 0]])\n",
    "\n",
    "P = np.array([[1, 0.5, 0.25],\n",
    "             [0.5, 1, 0.5],\n",
    "             [0.25, 0.5, 1]])\n",
    "\n",
    "mds_embedding_D = mdsD.fit_transform(D)\n",
    "mds_embedding_P = mdsP.fit_transform(P)\n",
    "\n",
    "stress_D = mdsD.stress_\n",
    "stress_P = mdsP.stress_\n",
    "\n",
    "print(mds_embedding_D)\n",
    "print(mds_embedding_P)\n",
    "\n",
    "print(stress_D)\n",
    "print(stress_P)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2 4]\n",
      " [2 0 2]\n",
      " [4 2 0]]\n",
      "[[1.         0.13533528 0.01831564]\n",
      " [0.13533528 1.         0.13533528]\n",
      " [0.01831564 0.13533528 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#d = D.shape[0]\n",
    "#D[range(d),range(d)] = 1\n",
    "\n",
    "exp_P = np.exp(-D)\n",
    "\n",
    "print(D)\n",
    "print(exp_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.13533528 0.01831564]\n",
      " [0.13533528 1.         0.13533528]\n",
      " [0.01831564 0.13533528 1.        ]]\n",
      "[[-0.28460756]\n",
      " [ 0.57528985]\n",
      " [-0.29473211]]\n",
      "0.005154604131453216\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "exp_P = np.exp(-D)\n",
    "print(exp_P)\n",
    "#d = D.shape[0]\n",
    "#D[range(d),range(d)] = 1\n",
    "\n",
    "mds_embedding_exP = mdsP.fit(exp_P).embedding_\n",
    "stress_exP = mdsP.stress_\n",
    "\n",
    "print(mds_embedding_exP)\n",
    "print(stress_exP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.03900655]\n",
      " [ 0.09022352]\n",
      " [-0.05121697]]\n"
     ]
    }
   ],
   "source": [
    "nmds_embedding_D = mdsD.fit_transform(exp_P)\n",
    "print(nmds_embedding_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.13533528 0.01831564]\n",
      " [0.13533528 1.         0.13533528]\n",
      " [0.01831564 0.13533528 1.        ]]\n",
      "[[-0.  2.  4.]\n",
      " [ 2. -0.  2.]\n",
      " [ 4.  2. -0.]]\n",
      "[[ 2.]\n",
      " [ 0.]\n",
      " [-2.]]\n",
      "0.005004649928775297\n"
     ]
    }
   ],
   "source": [
    "print(exp_P)\n",
    "ln_X = -np.log(exp_P)\n",
    "print(ln_X)\n",
    "\n",
    "mds_embedding_exP = mdsD.fit_transform(ln_X)\n",
    "stress_ln =  mdsD.stress_\n",
    "\n",
    "print(mds_embedding_exP)\n",
    "print(stress_exP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
